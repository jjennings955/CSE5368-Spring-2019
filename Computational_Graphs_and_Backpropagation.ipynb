{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computational Graphs and Backpropagation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjennings955/CSE5368-Spring-2019/blob/master/Computational_Graphs_and_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-kVCSnl0S5sJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Computational Graphs\n",
        "We can create a computational graph for any mathematical function\n",
        "\n",
        "For example:\n",
        "$f(x,y,z) = 2 x^2 cos(yz)$\n",
        "<pre>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</pre>\n",
        "\n",
        "or \n",
        "$f(x; W, b) = \\sigma(Wx + b)$\n",
        "<pre>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "metadata": {
        "id": "-PGTG9-0T_SQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We call $x, y, z$ **variables**, and the functions we apply to those variables **operations** (or functions). Variables and functions together we will call **nodes** in the graph.\n",
        "\n",
        "- **variables** - can be scalars, vectors, matrices, tensors, or any mathematical object (but we'll stick to those in this course)\n",
        "- **operations** can be any functions of those mathematical objects (so functions mapping scalars -> scalars, vectors -> vectors, or any other combination)\n",
        "\n",
        "Some libraries may add further classifications of **nodes** (such as 'placeholders' in tensorflow), but those are implementation details (important to tensorflow, but not important to the concept of a computational graph)\n",
        "\n",
        "Together these nodes and edges form a **directed acyclic graph** (or **DAG**), and we call this graph a computational graph."
      ]
    },
    {
      "metadata": {
        "id": "F6s45BP0VCSo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation\n",
        "Forward propagation is the process of iterating over the graph, feeding values into variable, and evaluating each operation, collecting the result, and feeding it into any remaining operations down stream, until we have fully evaluated the graph and computed the final output. It may also be called **feed forward** or **the forward pass**.\n",
        "\n",
        "We go through the graph in **topological order**. That is, we start by processing a node that has no predecessor in the graph, mark it as 'complete', and only process a node if all its predecessors have already been processed. A graph may have many valid topological orders.\n",
        "\n",
        "This is intuitively what we do when we evaluate a mathematical expression without necessarily realizing it."
      ]
    },
    {
      "metadata": {
        "id": "mqqF_UBvZDPg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Backpropagation\n",
        "Backward propagation is a process we implement to compute gradients in computational graphs. Because in deep learning we use **gradient based optimization**, a method for efficiently and automatically computing the gradient of a complex function is something desirable. \n",
        "\n",
        "Backpropagation in other fields may be called **reverse mode automatic differentiation**.\n",
        "\n",
        "In backward propagation, we iterate over the graph in **reverse topological order**, compute gradients, and feed those gradients backwards using **the chain rule**. You can think of backpropagation as simply a method for efficiently implementing the chain rule in multi-dimensions, for potentially very deeply nested chains of functions.\n",
        "\n",
        "## The Chain Rule\n",
        "In a single dimension, as in Calculus 101, the chain rule is simply as follows:\n",
        "Given:\n",
        "\n",
        "$y = f(x)$ and $x = g(t)$\n",
        "\n",
        "$$\\frac{dy}{dt} = \\frac{dy}{dx} \\frac{dx}{dt}$$\n",
        "\n",
        "In multi-dimensions (but still with scalar valued functions), it gets slightly more complex, because chains of functions may not be completely linear (like in the examples we've seen).\n",
        "\n",
        "Given $u = f(x,y)$ where $x = f_1(r,t)$ and $y = f_2(r,t)$\n",
        "\n",
        "$$\\frac{\\delta u}{\\delta t} = \\frac{\\delta u}{\\delta x} \\frac{\\delta x}{\\delta t} + \\frac{\\delta u}{\\delta y} \\frac{\\delta y }{\\delta t}$$\n",
        "\n",
        "In english, as we go backwards in a computational graph, we **multiply along linear chains**, and **add where two chains converge**.\n",
        "<pre>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</pre>\n",
        "\n",
        "\n",
        "## Backpropagation Motivation\n",
        "Rather than writing down a mathematical expression, symbolically computing an abstract formula for its gradient using calculus, and attempting to simplify it into something elegant, then implementing it into code, we can implement a library of general purpose operations, we just need to be able to define their **forward** behavior and their **backward** behavior.\n",
        "\n",
        "The forward behavior simply computes the output of the function (as we've seen), the backward behavior computes the gradient of the output with respect to its inputs.\n",
        "\n",
        "```python\n",
        "class Operation:\n",
        "  def forward(self, x):\n",
        "    return f(x)\n",
        "  def backward(self):\n",
        "    return gradient_f(x)\n",
        " ```\n"
      ]
    },
    {
      "metadata": {
        "id": "E8BjdAe6g6nJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vector Chain Rule\n",
        "Generally speaking for deep learning, most of our functions will be vector -> vector (or even tensor->tensor) functions. The rules for these can be derived from vector->scalar functions, but we'll skip the proof:\n",
        "\n",
        "$f : R^n \\rightarrow R^m$ \\\\\n",
        "\n",
        "You can think of a vector to vector function as $m$ functions of the vector $x$, stacked into a vector.\n",
        "\n",
        "$ f(\\textbf{x}) = \\begin{bmatrix} f_1(\\textbf{x}) \\\\ f_2(\\textbf{x}) \\\\ ... \\\\ f_m(\\textbf{x})\\end{bmatrix}$\n",
        "\n",
        "\n",
        "The Jacobian of f is defined as:\n",
        "\n",
        "$\\textbf{J}_f = \\begin{bmatrix}{\\dfrac {\\partial f_{1}}{\\partial x_{1}}}&\\cdots &{\\dfrac {\\partial f_{1}}{\\partial x_{n}}}\\\\\\vdots &\\ddots &\\vdots \\\\{\\dfrac {\\partial f_{m}}{\\partial x_{1}}}&\\cdots &{\\dfrac {\\partial f_{m}}{\\partial x_{n}}}\\end{bmatrix}$\n",
        "\n",
        "It can be seen that row $i$ of the Jacobian is the gradient (transposed) of $f_i(x)$ with respect to $\\textbf{x}$\n",
        "\n",
        "We introduce this rule, because it is used in the most general form of the chain rule (so far).\n",
        "\n",
        "$y = f(u) = (f_1(u), …, f_k(u))$ \n",
        "\n",
        "and \n",
        "\n",
        "$u = g(x) = (g_1(x), …, g_m(x))$\n"
      ]
    },
    {
      "metadata": {
        "id": "UE78vEk6hEhQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}