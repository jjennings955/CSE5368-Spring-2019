{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mean squared error Gradient.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjennings955/CSE5368-Spring-2019/blob/master/Mean_squared_error_Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "gkqQhEdm5PIP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Problem Formulation \n",
        "$X = \\{ x^{(1)}, x^{(2)}, ..., x^{(n)} \\}$\n",
        "\n",
        "$Y = \\{ y^{(1)}, y^{(2)}, ..., y^{(n)} \\}$\n",
        "\n",
        "\n",
        "$\\hat y = w^T x + b$\n",
        "\n",
        "$p(y|x) = N(y; \\mu = \\hat y, \\sigma^2)$\n",
        "\n",
        "\n",
        "Using this setup, we derived Mean Squared Error from maximum likelihood (derivation omitted)\n",
        "\n",
        "# Objective function\n",
        "$MSE = \\frac{1}{N} \\sum_{i} ||\\hat y^{(i)} - y^{(i)}||^2$\n",
        "\n",
        "# 2D Example - Formulation as Optimization Problem\n",
        "$w = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}$\n",
        "$x^{(i)} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$\n",
        "\n",
        "$w*,b* = \\text{argmin}_{w,b} \\frac{1}{N} \\sum_{i} ||\\hat y^{(i)} - y^{(i)}||^2$\n",
        "\n",
        "In english: find the weights and bias values that minimize our mean squared error\n",
        "\n",
        "# Substitute $\\hat y$ for its equation\n",
        "\n",
        "$w*,b* = \\text{argmin}_{w,b} \\frac{1}{N} \\sum_{i} ||w^T x^{(i)} + b - y^{(i)}||^2$\n",
        "\n",
        "# The gradient of a sum is the sum of the gradients\n",
        "\n",
        "From calculus I: $\\frac{d}{dx} (a+b) = \\frac{d}{dx} (a) + \\frac{d}{dx}(b)$\n",
        "\n",
        "The same is true for partial derivatives $\\frac{\\delta}{\\delta x} (a+b) = \\frac{\\delta}{\\delta x} (a) + \\frac{\\delta}{\\delta x}(b)$\n",
        "\n",
        "And for gradient $\\nabla_x (a+b) = \\nabla_x (a) + \\nabla_x(b)$\n",
        "\n",
        "This implies $ \\nabla_x [\\sum_i  f(x)]= \\sum_i \\nabla_x [ f(x) ]$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IpQLIvCFJbb4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Complete gradient calculation\n",
        "Using this, we obtain:\n",
        "\n",
        "$\\nabla_{w,b} (\\frac{1}{N} \\sum_{i} ||w^T x^{(i)} + b - y^{(i)}||^2) =  \\frac{1}{N} \\sum_{i} \\nabla_{w,b} [w^T x^{(i)} + b - y^{(i)} ]^2 $\n",
        "\n",
        "We solve this for a single term of the summation to determine a formula.\n",
        "\n",
        "Since we are solving this for a 2D example, we expand the dot product ($w^T x = w_1 x_1 + w_2 x_2$)\n",
        "\n",
        "$\\nabla_{w,b} [w^T x^{(i)} + b - y^{(i)} ]^2 = \\nabla_w [w_1 x^{(1)}_1 + w_2 x^{(1)}_2 + b - y^{(i)} ]^2$\n",
        "\n",
        "The gradient for a single term in this summation is calculated below using the chain rule:\n",
        "\n",
        "$\\frac{\\delta}{\\delta w_1}[w_1 x^{(1)}_1 + w_2 x^{(1)}_2 + b - y^{(i)} ]^2 = 2[w_1 x^{(1)}_1 + w_2 x^{(1)}_2 + b - y^{(i)} ] x^{(1)}_0$\n",
        "\n",
        "$\\frac{\\delta}{\\delta w_1}[w_1 x^{(1)}_1 + w_2 x^{(1)}_2 + b - y^{(i)} ]^2 = 2[w_1 x^{(1)}_1 + w_2 x^{(1)}_2 + b - y^{(i)} ] x^{(1)}_1$\n",
        "\n",
        "$\\frac{\\delta}{\\delta b}[w_1 x^{(1)}_1 + w_2 x^{(1)}_2 + b - y^{(i)} ]^2 = 2[w_1 x^{(1)}_1 + w_2 x^{(1)}_2 + b - y^{(i)} ] $\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0WfB2awU5Os5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simplification\n",
        "\n",
        "We can define our 'error':\n",
        "\n",
        "$E(x, y) = (w^T x + b - y)$\n",
        "\n",
        "Then our mean squared error is simply:\n",
        "\n",
        "$\\text{MSE} = \\frac{1}{N} \\sum_{i} E(x^{(i)}, y^{(i)})^2 $\n",
        "\n",
        "Using the chain rule $f(g(x))' = f'(g(x))g'(x)$\n",
        "\n",
        "Consider $g(x) = E(x,y); f(x) = x^2$\n",
        "\n",
        "$\\nabla_w \\text{MSE} = \\begin{bmatrix} \\frac{\\delta}{\\delta w_0} \\\\ \\frac{\\delta}{\\delta w_1} \\end{bmatrix} = \\frac{1}{N} \\sum_{i} [2 E(x^{(i)}, y^{(i)}) *  x^{(i)}] $\n",
        "\n",
        "And similarly for the bias:\n",
        "\n",
        "$\\frac{\\delta}{\\delta b} = \\frac{1}{N} \\sum_{i} [2 E(x^{(i)}, y^{(i)}) * 1] $\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AgtseGbb5OCB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Further simplification ('the bias trick')\n",
        "\n",
        "Some texts choose to place the bias in the vector with the weights, using the following trick\n",
        "\n",
        "$w = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ b \\end{bmatrix}$\n",
        "$x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ 1 \\end{bmatrix}$\n",
        "\n",
        "Now our model simply becomes\n",
        "\n",
        "$\\hat y = w^T x$\n",
        "\n",
        "And our gradient becomes unified as follows:\n",
        "\n",
        "$\\nabla_w \\text{MSE} = \\begin{bmatrix} \\frac{\\delta}{\\delta w_0} \\\\ \\frac{\\delta}{\\delta w_1} \\\\ \\frac{\\delta}{\\delta b} \\end{bmatrix} = \\frac{1}{N} \\sum_{i} [2 E(x^{(i)}, y^{(i)}) *  x^{(i)}] $"
      ]
    },
    {
      "metadata": {
        "id": "3LS0G5xKJJl0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Some clarifications\n",
        "\n",
        "In some cases I may have swapped the order of $y$ and $\\hat y$ in the $\\text{MSE}$ formula, it doesn't actually make any difference.\n",
        "\n",
        "$\\text{MSE} = \\frac{1}{N} \\sum_{i} ||\\hat y^{(i)} - y^{(i)}||^2$  vs $\\text{MSE} = \\frac{1}{N} \\sum_{i} || y^{(i)} - \\hat y^{(i)}||^2$\n",
        "\n",
        "You can illustrate this by expanding \n",
        "$(\\hat y - y)^2$ and  $(y - \\hat y)^2$\n",
        "\n",
        "In the first you get $\\hat y^2 - 2 \\hat y y + y^2$ in the second you get $y^2 - 2 \\hat y y +\\hat y^2$, and these are equivalent.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yMUuR34RJICp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Some other variations\n",
        "In many texts or deep learning libraries, the convention is to write data as row vectors (like you would in an excel spreadsheet), where each row represents 1 of n **samples**, and each column represents 1 of m **features** of that sample.\n",
        "\n",
        "So $x^{(1)} = \\begin{bmatrix} x^{(1)}_1 & x^{(1)}_2 & ... & x^{(1)}_m \\end{bmatrix}$ represents a single sample, the attributes of a single person for example, $x^{(1)}_1$ might be that person's height and $x^{(1)}_2$ that person's weight, etc\n",
        "\n",
        "Using this convention, our model becomes\n",
        "\n",
        "$\\hat y = x w$\n",
        "\n",
        "Additionally, this allows us to stack all of our data (or a batch of data for **Stochastic Gradient Descent**) into a single matrix and efficiently compute our model outputs using a single matrix multiplication. You may also use the bias trick in this way by having an entire column of 1's.\n",
        "\n",
        "$X = \\begin{bmatrix} \n",
        "  x^{(1)}_1 & x^{(1)}_2 & ... & x^{(1)}_m & 1 \\\\\n",
        "  x^{(2)}_1 & x^{(2)}_2 & ... & x^{(2)}_m & 1 \\\\\n",
        "  ... & ... & ... & ... & 1 \\\\\n",
        "  x^{(n)}_1 & x^{(n)}_2 & ... & x^{(n)}_m & 1  \\\\\n",
        "  \\end{bmatrix}\n",
        "$\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xDDe6ENiOOMb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " We may then compute our model output for all n samples by simply computing the following matrix-vector product\n",
        " \n",
        "$\\hat Y = Xw$\n",
        "\n",
        "Our result will be a $[n, 1]$ vector\n",
        "\n",
        "This is the more common convention in deep learning implementations, however I consider this an *implementation detail*, and we will likely continue using $w^T x + b$ throughout the course."
      ]
    },
    {
      "metadata": {
        "id": "d0ibGs8cOMjF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}